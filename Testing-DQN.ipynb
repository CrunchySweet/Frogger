{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae0a1e9-eddc-4625-ad93-0a7aba4b3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada4a3d5-91ea-4efe-abf2-a67fa06b6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale_py in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from ale_py) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5be514-3f69-491e-b172-2ba7dbd662dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (4.13.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.10.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e4f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6091137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB data to grayscale. Keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9114e015-c9eb-451b-bdf6-bc8a8d714e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"CNN architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdae6fe6-eb82-45e8-8595-4f69114e5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System for progressively saving weights as the CNN trains\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc3db75-c6d8-4986-80aa-88d8ee4ed17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy -- balances exploration and exploitation\n",
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.random() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose random action\n",
    "    else:\n",
    "        # Exploit: choose best action\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)  # Choose action with highest Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb77a63-ffbb-4202-a802-ff9c682b8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=10000,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a DQN model on Frogger with reward shaping to encourage faster completion.\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create main and target models\n",
    "    main_model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    \n",
    "    # Experience replay memory\n",
    "    memory = create_memory(capacity=memory_capacity)\n",
    "    \n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for episode in range(1, episodes + 1):\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "\n",
    "        episode_reward = 0\n",
    "        previous_row = info.get(\"player_y\", 0)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_action(main_model, state, epsilon, action_size)\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "\n",
    "            # --- Custom reward shaping ---\n",
    "            time_penalty = -0.01\n",
    "            completion_bonus = 50 if terminated and reward > 0 else 0\n",
    "\n",
    "            current_row = info.get(\"player_y\", previous_row)\n",
    "            progress_reward = (previous_row - current_row) * 0.1  # Encourage upward movement\n",
    "            previous_row = current_row\n",
    "\n",
    "            # Adjust the reward\n",
    "            reward += time_penalty + completion_bonus + progress_reward\n",
    "            # --------------------------------\n",
    "\n",
    "            add_to_memory(memory, state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = sample_from_memory(memory, batch_size)\n",
    "\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "\n",
    "                target_q_values = main_model.predict(states, verbose=0)\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        target_q_values[i, actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target_q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "\n",
    "                main_model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "\n",
    "            if total_steps % update_target_freq == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"Target network updated at step {total_steps}\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward:.2f}, Epsilon: {epsilon:.4f}, Steps: {step+1}\")\n",
    "\n",
    "        if episode % save_freq == 0:\n",
    "            main_model.save(f\"{save_dir}/frogger_dqn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    main_model.save(f\"{save_dir}/frogger_dqn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d68dd17-e235-4ef6-8d2b-8f29c4b02f9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# You can adjust these parameters as needed\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     train_dqn(episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m      5\u001b[0m               max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, \n\u001b[0;32m      6\u001b[0m               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m      7\u001b[0m               gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, \n\u001b[0;32m      8\u001b[0m               epsilon_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[0;32m      9\u001b[0m               epsilon_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \n\u001b[0;32m     10\u001b[0m               epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.995\u001b[39m,\n\u001b[0;32m     11\u001b[0m               update_target_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     12\u001b[0m               memory_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m,\n\u001b[0;32m     13\u001b[0m               save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dqn' is not defined"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as needed\n",
    "    train_dqn(episodes=1000,\n",
    "              max_steps=10000, \n",
    "              batch_size=1, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.01, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=1000,\n",
    "              memory_capacity=50000,\n",
    "              save_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ddd07-55d3-4644-be76-073cac91ef78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
