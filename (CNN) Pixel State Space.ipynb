{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d355c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgraham\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\bgraham\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76af5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5')\n",
    "frame, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a447062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN model for Q-learning\n",
    "def build_model(action_size):\n",
    "    \"\"\"Build a CNN model for Deep Q-Learning\"\"\"\n",
    "    model = Sequential()\n",
    "    # Input shape: grayscale image of 210x160 (210, 160, 1)\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))  # Output layer with one node per action\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c262f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 8.0, Epsilon: 0.9950, Steps: 328\n",
      "Episode: 2, Reward: 11.0, Epsilon: 0.9900, Steps: 303\n",
      "Episode: 3, Reward: 9.0, Epsilon: 0.9851, Steps: 345\n",
      "Target network updated at step 1000\n",
      "Episode: 4, Reward: 8.0, Epsilon: 0.9801, Steps: 209\n",
      "Episode: 5, Reward: 9.0, Epsilon: 0.9752, Steps: 229\n",
      "Episode: 6, Reward: 10.0, Epsilon: 0.9704, Steps: 279\n",
      "Episode: 7, Reward: 6.0, Epsilon: 0.9655, Steps: 297\n",
      "Target network updated at step 2000\n",
      "Episode: 8, Reward: 8.0, Epsilon: 0.9607, Steps: 229\n",
      "Episode: 9, Reward: 10.0, Epsilon: 0.9559, Steps: 302\n",
      "Episode: 10, Reward: 10.0, Epsilon: 0.9511, Steps: 283\n",
      "Target network updated at step 3000\n",
      "Episode: 11, Reward: 10.0, Epsilon: 0.9464, Steps: 251\n",
      "Episode: 12, Reward: 9.0, Epsilon: 0.9416, Steps: 226\n",
      "Episode: 13, Reward: 10.0, Epsilon: 0.9369, Steps: 299\n",
      "Episode: 14, Reward: 10.0, Epsilon: 0.9322, Steps: 273\n",
      "Target network updated at step 4000\n",
      "Episode: 15, Reward: 11.0, Epsilon: 0.9276, Steps: 247\n",
      "Episode: 16, Reward: 9.0, Epsilon: 0.9229, Steps: 275\n",
      "Episode: 17, Reward: 9.0, Epsilon: 0.9183, Steps: 244\n",
      "Episode: 18, Reward: 9.0, Epsilon: 0.9137, Steps: 276\n",
      "Target network updated at step 5000\n",
      "Episode: 19, Reward: 8.0, Epsilon: 0.9092, Steps: 234\n",
      "Episode: 20, Reward: 13.0, Epsilon: 0.9046, Steps: 293\n",
      "Episode: 21, Reward: 10.0, Epsilon: 0.9001, Steps: 257\n",
      "Episode: 22, Reward: 13.0, Epsilon: 0.8956, Steps: 308\n",
      "Target network updated at step 6000\n",
      "Episode: 23, Reward: 13.0, Epsilon: 0.8911, Steps: 294\n",
      "Episode: 24, Reward: 9.0, Epsilon: 0.8867, Steps: 237\n",
      "Episode: 25, Reward: 8.0, Epsilon: 0.8822, Steps: 217\n",
      "Target network updated at step 7000\n",
      "Episode: 26, Reward: 8.0, Epsilon: 0.8778, Steps: 276\n",
      "Episode: 27, Reward: 12.0, Epsilon: 0.8734, Steps: 231\n",
      "Episode: 28, Reward: 8.0, Epsilon: 0.8691, Steps: 235\n",
      "Episode: 29, Reward: 13.0, Epsilon: 0.8647, Steps: 280\n",
      "Target network updated at step 8000\n",
      "Episode: 30, Reward: 12.0, Epsilon: 0.8604, Steps: 355\n",
      "Episode: 31, Reward: 15.0, Epsilon: 0.8561, Steps: 272\n",
      "Episode: 32, Reward: 11.0, Epsilon: 0.8518, Steps: 278\n",
      "Episode: 33, Reward: 8.0, Epsilon: 0.8475, Steps: 224\n",
      "Target network updated at step 9000\n",
      "Episode: 34, Reward: 9.0, Epsilon: 0.8433, Steps: 291\n",
      "Episode: 35, Reward: 16.0, Epsilon: 0.8391, Steps: 292\n",
      "Episode: 36, Reward: 11.0, Epsilon: 0.8349, Steps: 282\n",
      "Episode: 37, Reward: 9.0, Epsilon: 0.8307, Steps: 237\n",
      "Target network updated at step 10000\n",
      "Episode: 38, Reward: 7.0, Epsilon: 0.8266, Steps: 232\n",
      "Episode: 39, Reward: 7.0, Epsilon: 0.8224, Steps: 235\n",
      "Episode: 40, Reward: 10.0, Epsilon: 0.8183, Steps: 281\n",
      "Target network updated at step 11000\n",
      "Episode: 41, Reward: 14.0, Epsilon: 0.8142, Steps: 377\n",
      "Episode: 42, Reward: 10.0, Epsilon: 0.8102, Steps: 298\n",
      "Episode: 43, Reward: 9.0, Epsilon: 0.8061, Steps: 239\n",
      "Episode: 44, Reward: 8.0, Epsilon: 0.8021, Steps: 232\n",
      "Target network updated at step 12000\n",
      "Episode: 45, Reward: 9.0, Epsilon: 0.7981, Steps: 217\n",
      "Episode: 46, Reward: 10.0, Epsilon: 0.7941, Steps: 285\n",
      "Episode: 47, Reward: 10.0, Epsilon: 0.7901, Steps: 301\n",
      "Episode: 48, Reward: 9.0, Epsilon: 0.7862, Steps: 253\n",
      "Target network updated at step 13000\n",
      "Episode: 49, Reward: 8.0, Epsilon: 0.7822, Steps: 262\n",
      "Episode: 50, Reward: 10.0, Epsilon: 0.7783, Steps: 233\n",
      "Model saved at episode 50\n",
      "Episode: 51, Reward: 14.0, Epsilon: 0.7744, Steps: 272\n",
      "Episode: 52, Reward: 9.0, Epsilon: 0.7705, Steps: 249\n",
      "Target network updated at step 14000\n",
      "Episode: 53, Reward: 15.0, Epsilon: 0.7667, Steps: 294\n",
      "Episode: 54, Reward: 17.0, Epsilon: 0.7629, Steps: 265\n",
      "Episode: 55, Reward: 14.0, Epsilon: 0.7590, Steps: 286\n",
      "Target network updated at step 15000\n",
      "Episode: 56, Reward: 21.0, Epsilon: 0.7553, Steps: 325\n",
      "Episode: 57, Reward: 17.0, Epsilon: 0.7515, Steps: 288\n",
      "Episode: 58, Reward: 16.0, Epsilon: 0.7477, Steps: 291\n",
      "Episode: 59, Reward: 10.0, Epsilon: 0.7440, Steps: 275\n",
      "Target network updated at step 16000\n",
      "Episode: 60, Reward: 15.0, Epsilon: 0.7403, Steps: 285\n",
      "Episode: 61, Reward: 10.0, Epsilon: 0.7366, Steps: 268\n",
      "Episode: 62, Reward: 9.0, Epsilon: 0.7329, Steps: 280\n",
      "Target network updated at step 17000\n",
      "Episode: 63, Reward: 8.0, Epsilon: 0.7292, Steps: 254\n",
      "Episode: 64, Reward: 9.0, Epsilon: 0.7256, Steps: 261\n",
      "Episode: 65, Reward: 11.0, Epsilon: 0.7219, Steps: 255\n",
      "Episode: 66, Reward: 12.0, Epsilon: 0.7183, Steps: 246\n",
      "Target network updated at step 18000\n",
      "Episode: 67, Reward: 8.0, Epsilon: 0.7147, Steps: 228\n",
      "Episode: 68, Reward: 14.0, Epsilon: 0.7112, Steps: 269\n",
      "Episode: 69, Reward: 11.0, Epsilon: 0.7076, Steps: 284\n",
      "Episode: 70, Reward: 13.0, Epsilon: 0.7041, Steps: 274\n",
      "Target network updated at step 19000\n",
      "Episode: 71, Reward: 15.0, Epsilon: 0.7005, Steps: 299\n",
      "Episode: 72, Reward: 12.0, Epsilon: 0.6970, Steps: 295\n",
      "Episode: 73, Reward: 17.0, Epsilon: 0.6936, Steps: 273\n",
      "Episode: 74, Reward: 8.0, Epsilon: 0.6901, Steps: 226\n",
      "Target network updated at step 20000\n",
      "Episode: 75, Reward: 13.0, Epsilon: 0.6866, Steps: 263\n",
      "Episode: 76, Reward: 11.0, Epsilon: 0.6832, Steps: 278\n",
      "Episode: 77, Reward: 12.0, Epsilon: 0.6798, Steps: 265\n",
      "Target network updated at step 21000\n",
      "Episode: 78, Reward: 9.0, Epsilon: 0.6764, Steps: 240\n",
      "Episode: 79, Reward: 11.0, Epsilon: 0.6730, Steps: 340\n",
      "Episode: 80, Reward: 16.0, Epsilon: 0.6696, Steps: 313\n",
      "Episode: 81, Reward: 10.0, Epsilon: 0.6663, Steps: 281\n",
      "Target network updated at step 22000\n",
      "Episode: 82, Reward: 19.0, Epsilon: 0.6630, Steps: 370\n",
      "Episode: 83, Reward: 18.0, Epsilon: 0.6597, Steps: 328\n",
      "Episode: 84, Reward: 14.0, Epsilon: 0.6564, Steps: 283\n",
      "Target network updated at step 23000\n",
      "Episode: 85, Reward: 15.0, Epsilon: 0.6531, Steps: 276\n",
      "Episode: 86, Reward: 18.0, Epsilon: 0.6498, Steps: 271\n",
      "Episode: 87, Reward: 13.0, Epsilon: 0.6466, Steps: 297\n",
      "Target network updated at step 24000\n",
      "Episode: 88, Reward: 13.0, Epsilon: 0.6433, Steps: 226\n",
      "Episode: 89, Reward: 8.0, Epsilon: 0.6401, Steps: 256\n",
      "Episode: 90, Reward: 9.0, Epsilon: 0.6369, Steps: 272\n",
      "Episode: 91, Reward: 16.0, Epsilon: 0.6337, Steps: 278\n",
      "Target network updated at step 25000\n",
      "Episode: 92, Reward: 8.0, Epsilon: 0.6306, Steps: 217\n",
      "Episode: 93, Reward: 14.0, Epsilon: 0.6274, Steps: 293\n",
      "Episode: 94, Reward: 18.0, Epsilon: 0.6243, Steps: 300\n",
      "Episode: 95, Reward: 15.0, Epsilon: 0.6211, Steps: 322\n",
      "Target network updated at step 26000\n",
      "Episode: 96, Reward: 19.0, Epsilon: 0.6180, Steps: 278\n",
      "Episode: 97, Reward: 15.0, Epsilon: 0.6149, Steps: 372\n",
      "Episode: 98, Reward: 14.0, Epsilon: 0.6119, Steps: 328\n",
      "Target network updated at step 27000\n",
      "Episode: 99, Reward: 10.0, Epsilon: 0.6088, Steps: 211\n",
      "Episode: 100, Reward: 14.0, Epsilon: 0.6058, Steps: 333\n",
      "Model saved at episode 100\n",
      "Episode: 101, Reward: 11.0, Epsilon: 0.6027, Steps: 274\n",
      "Target network updated at step 28000\n",
      "Episode: 102, Reward: 11.0, Epsilon: 0.5997, Steps: 283\n",
      "Episode: 103, Reward: 12.0, Epsilon: 0.5967, Steps: 297\n",
      "Episode: 104, Reward: 9.0, Epsilon: 0.5937, Steps: 265\n",
      "Episode: 105, Reward: 11.0, Epsilon: 0.5908, Steps: 297\n",
      "Target network updated at step 29000\n",
      "Episode: 106, Reward: 9.0, Epsilon: 0.5878, Steps: 251\n",
      "Episode: 107, Reward: 19.0, Epsilon: 0.5849, Steps: 338\n",
      "Episode: 108, Reward: 17.0, Epsilon: 0.5820, Steps: 309\n",
      "Target network updated at step 30000\n",
      "Episode: 109, Reward: 10.0, Epsilon: 0.5790, Steps: 269\n",
      "Episode: 110, Reward: 15.0, Epsilon: 0.5762, Steps: 284\n",
      "Episode: 111, Reward: 12.0, Epsilon: 0.5733, Steps: 306\n",
      "Episode: 112, Reward: 16.0, Epsilon: 0.5704, Steps: 348\n",
      "Target network updated at step 31000\n",
      "Episode: 113, Reward: 13.0, Epsilon: 0.5676, Steps: 282\n",
      "Episode: 114, Reward: 11.0, Epsilon: 0.5647, Steps: 224\n",
      "Episode: 115, Reward: 12.0, Epsilon: 0.5619, Steps: 275\n",
      "Target network updated at step 32000\n",
      "Episode: 116, Reward: 14.0, Epsilon: 0.5591, Steps: 284\n",
      "Episode: 117, Reward: 22.0, Epsilon: 0.5563, Steps: 385\n",
      "Episode: 118, Reward: 23.0, Epsilon: 0.5535, Steps: 371\n",
      "Target network updated at step 33000\n",
      "Episode: 119, Reward: 12.0, Epsilon: 0.5507, Steps: 244\n",
      "Episode: 120, Reward: 14.0, Epsilon: 0.5480, Steps: 281\n",
      "Episode: 121, Reward: 13.0, Epsilon: 0.5452, Steps: 305\n",
      "Episode: 122, Reward: 13.0, Epsilon: 0.5425, Steps: 296\n",
      "Target network updated at step 34000\n",
      "Episode: 123, Reward: 17.0, Epsilon: 0.5398, Steps: 346\n",
      "Episode: 124, Reward: 18.0, Epsilon: 0.5371, Steps: 295\n",
      "Episode: 125, Reward: 21.0, Epsilon: 0.5344, Steps: 332\n",
      "Target network updated at step 35000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 126, Reward: 18.0, Epsilon: 0.5318, Steps: 390\n",
      "Episode: 127, Reward: 13.0, Epsilon: 0.5291, Steps: 271\n",
      "Episode: 128, Reward: 11.0, Epsilon: 0.5264, Steps: 388\n",
      "Target network updated at step 36000\n",
      "Episode: 129, Reward: 18.0, Epsilon: 0.5238, Steps: 330\n",
      "Episode: 130, Reward: 11.0, Epsilon: 0.5212, Steps: 286\n",
      "Episode: 131, Reward: 11.0, Epsilon: 0.5186, Steps: 270\n",
      "Target network updated at step 37000\n",
      "Episode: 132, Reward: 14.0, Epsilon: 0.5160, Steps: 252\n",
      "Episode: 133, Reward: 17.0, Epsilon: 0.5134, Steps: 280\n",
      "Episode: 134, Reward: 19.0, Epsilon: 0.5108, Steps: 342\n",
      "Target network updated at step 38000\n",
      "Episode: 135, Reward: 18.0, Epsilon: 0.5083, Steps: 411\n",
      "Episode: 136, Reward: 20.0, Epsilon: 0.5058, Steps: 441\n",
      "Episode: 137, Reward: 9.0, Epsilon: 0.5032, Steps: 388\n",
      "Target network updated at step 39000\n",
      "Episode: 138, Reward: 21.0, Epsilon: 0.5007, Steps: 305\n",
      "Episode: 139, Reward: 18.0, Epsilon: 0.4982, Steps: 445\n",
      "Target network updated at step 40000\n",
      "Episode: 140, Reward: 16.0, Epsilon: 0.4957, Steps: 299\n",
      "Episode: 141, Reward: 19.0, Epsilon: 0.4932, Steps: 289\n",
      "Episode: 142, Reward: 17.0, Epsilon: 0.4908, Steps: 373\n",
      "Target network updated at step 41000\n",
      "Episode: 143, Reward: 15.0, Epsilon: 0.4883, Steps: 361\n",
      "Episode: 144, Reward: 21.0, Epsilon: 0.4859, Steps: 387\n",
      "Episode: 145, Reward: 19.0, Epsilon: 0.4834, Steps: 361\n",
      "Target network updated at step 42000\n",
      "Episode: 146, Reward: 17.0, Epsilon: 0.4810, Steps: 380\n",
      "Episode: 147, Reward: 15.0, Epsilon: 0.4786, Steps: 391\n",
      "Episode: 148, Reward: 18.0, Epsilon: 0.4762, Steps: 304\n",
      "Target network updated at step 43000\n",
      "Episode: 149, Reward: 19.0, Epsilon: 0.4738, Steps: 465\n",
      "Episode: 150, Reward: 13.0, Epsilon: 0.4715, Steps: 263\n",
      "Model saved at episode 150\n",
      "Episode: 151, Reward: 9.0, Epsilon: 0.4691, Steps: 396\n",
      "Target network updated at step 44000\n",
      "Episode: 152, Reward: 15.0, Epsilon: 0.4668, Steps: 375\n",
      "Episode: 153, Reward: 12.0, Epsilon: 0.4644, Steps: 374\n",
      "Target network updated at step 45000\n",
      "Episode: 154, Reward: 21.0, Epsilon: 0.4621, Steps: 465\n",
      "Episode: 155, Reward: 21.0, Epsilon: 0.4598, Steps: 396\n",
      "Target network updated at step 46000\n",
      "Episode: 156, Reward: 12.0, Epsilon: 0.4575, Steps: 448\n",
      "Episode: 157, Reward: 21.0, Epsilon: 0.4552, Steps: 378\n",
      "Episode: 158, Reward: 23.0, Epsilon: 0.4529, Steps: 364\n",
      "Target network updated at step 47000\n",
      "Episode: 159, Reward: 16.0, Epsilon: 0.4507, Steps: 317\n",
      "Episode: 160, Reward: 15.0, Epsilon: 0.4484, Steps: 392\n",
      "Episode: 161, Reward: 25.0, Epsilon: 0.4462, Steps: 376\n",
      "Target network updated at step 48000\n",
      "Episode: 162, Reward: 19.0, Epsilon: 0.4440, Steps: 298\n",
      "Episode: 163, Reward: 14.0, Epsilon: 0.4417, Steps: 314\n",
      "Episode: 164, Reward: 8.0, Epsilon: 0.4395, Steps: 300\n",
      "Target network updated at step 49000\n",
      "Episode: 165, Reward: 16.0, Epsilon: 0.4373, Steps: 272\n",
      "Episode: 166, Reward: 16.0, Epsilon: 0.4351, Steps: 417\n",
      "Episode: 167, Reward: 10.0, Epsilon: 0.4330, Steps: 321\n",
      "Target network updated at step 50000\n",
      "Episode: 168, Reward: 12.0, Epsilon: 0.4308, Steps: 263\n",
      "Episode: 169, Reward: 21.0, Epsilon: 0.4286, Steps: 387\n",
      "Episode: 170, Reward: 24.0, Epsilon: 0.4265, Steps: 431\n",
      "Target network updated at step 51000\n",
      "Episode: 171, Reward: 22.0, Epsilon: 0.4244, Steps: 374\n",
      "Episode: 172, Reward: 20.0, Epsilon: 0.4223, Steps: 436\n",
      "Target network updated at step 52000\n",
      "Episode: 173, Reward: 21.0, Epsilon: 0.4201, Steps: 356\n",
      "Episode: 174, Reward: 19.0, Epsilon: 0.4180, Steps: 434\n",
      "Episode: 175, Reward: 14.0, Epsilon: 0.4159, Steps: 257\n",
      "Target network updated at step 53000\n",
      "Episode: 176, Reward: 14.0, Epsilon: 0.4139, Steps: 310\n",
      "Episode: 177, Reward: 15.0, Epsilon: 0.4118, Steps: 273\n",
      "Episode: 178, Reward: 12.0, Epsilon: 0.4097, Steps: 322\n",
      "Episode: 179, Reward: 13.0, Epsilon: 0.4077, Steps: 353\n",
      "Target network updated at step 54000\n",
      "Episode: 180, Reward: 17.0, Epsilon: 0.4057, Steps: 302\n",
      "Episode: 181, Reward: 11.0, Epsilon: 0.4036, Steps: 389\n",
      "Episode: 182, Reward: 21.0, Epsilon: 0.4016, Steps: 324\n",
      "Target network updated at step 55000\n",
      "Episode: 183, Reward: 21.0, Epsilon: 0.3996, Steps: 381\n",
      "Episode: 184, Reward: 22.0, Epsilon: 0.3976, Steps: 375\n",
      "Target network updated at step 56000\n",
      "Episode: 185, Reward: 18.0, Epsilon: 0.3956, Steps: 515\n",
      "Episode: 186, Reward: 13.0, Epsilon: 0.3936, Steps: 268\n",
      "Episode: 187, Reward: 21.0, Epsilon: 0.3917, Steps: 321\n",
      "Target network updated at step 57000\n",
      "Episode: 188, Reward: 20.0, Epsilon: 0.3897, Steps: 319\n",
      "Episode: 189, Reward: 23.0, Epsilon: 0.3878, Steps: 445\n",
      "Episode: 190, Reward: 16.0, Epsilon: 0.3858, Steps: 319\n",
      "Target network updated at step 58000\n",
      "Episode: 191, Reward: 10.0, Epsilon: 0.3839, Steps: 228\n",
      "Episode: 192, Reward: 20.0, Epsilon: 0.3820, Steps: 341\n",
      "Episode: 193, Reward: 13.0, Epsilon: 0.3801, Steps: 300\n",
      "Target network updated at step 59000\n",
      "Episode: 194, Reward: 24.0, Epsilon: 0.3782, Steps: 318\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB frame to grayscale and keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Keep original dimensions\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame\n",
    "\n",
    "\n",
    "# Experience replay memory\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size)\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.random() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose random action\n",
    "    else:\n",
    "        # Exploit: choose best action\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)  # Choose action with highest Q-value\n",
    "\n",
    "# Training function\n",
    "def train_dqn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=10000,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a DQN model on Frogger\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create main and target models\n",
    "    main_model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(main_model.get_weights())  # Initialize target with same weights\n",
    "    \n",
    "    # Create memory for experience replay\n",
    "    memory = create_memory(capacity=memory_capacity)\n",
    "    \n",
    "    # Training metrics\n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Model saving directory\n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, episodes + 1):\n",
    "        # Reset environment and get initial state\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension: (120, 160, 1)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = epsilon_greedy_action(main_model, state, epsilon, action_size)\n",
    "            \n",
    "            # Take action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)  # Add channel dimension\n",
    "            \n",
    "            # Store experience in memory\n",
    "            add_to_memory(memory, state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Train with experience replay if memory has enough samples\n",
    "            if len(memory) > batch_size:\n",
    "                # Sample batch from memory\n",
    "                minibatch = sample_from_memory(memory, batch_size)\n",
    "                \n",
    "                # Prepare batch for training\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                target_q_values = main_model.predict(states, verbose=0)\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        target_q_values[i, actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target_q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "                \n",
    "                # Train the model\n",
    "                main_model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "            \n",
    "            # Update target network periodically\n",
    "            if total_steps % update_target_freq == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"Target network updated at step {total_steps}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}, Steps: {step+1}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % save_freq == 0:\n",
    "            main_model.save(f\"{save_dir}/frogger_dqn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    # Save final model\n",
    "    main_model.save(f\"{save_dir}/frogger_dqn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return main_model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as needed\n",
    "    train_dqn(episodes=1000,\n",
    "              max_steps=10000, \n",
    "              batch_size=8, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.01, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=1000,\n",
    "              memory_capacity=50000,\n",
    "              save_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69dd31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB frame to grayscale and keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Keep original dimensions\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a178f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"Build a CNN model for Deep Q-Learning\"\"\"\n",
    "    model = Sequential()\n",
    "    # Input shape: grayscale image of 210x160 (210, 160, 1)\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))  # Output layer with one node per action\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4886457",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'frogger_dqn_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2636\\1958948792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Assuming you have saved your trained model in 'frogger_model/frogger_dqn_final.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mrun_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ALE/Frogger-v5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'frogger_dqn_final.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2636\\1958948792.py\u001b[0m in \u001b[0;36mrun_trained_model\u001b[1;34m(env_name, model_path, episodes)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[1;32m--> 533\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'frogger_dqn_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "def run_trained_model(env_name, model_path, episodes=5):\n",
    "    # Initialize the environment\n",
    "    env = gym.make(env_name, render_mode='human' )\n",
    "    model = build_model(env.action_space.n)\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Render the environment to see the agent in action\n",
    "            time.sleep(0.01)  # Slow down simulation for better visualization\n",
    "\n",
    "            # Predict action from the model\n",
    "            q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "            action = np.argmax(q_values[0])  # Choose action with highest predicted Q-value\n",
    "\n",
    "            # Take the action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "\n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1} - Total Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Assuming you have saved your trained model in 'frogger_model/frogger_dqn_final.h5'\n",
    "run_trained_model(env_name='ALE/Frogger-v5', model_path='frogger_dqn_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d16d29d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'frogger_dqn_episode_200.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2636\\1132851380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'frogger_dqn_episode_200.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Run the game with trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[1;32m--> 533\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'frogger_dqn_episode_200.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model_path = 'frogger_dqn_episode_200.h5'\n",
    "model = build_model(action_size=5) \n",
    "model.load_weights(model_path)\n",
    "\n",
    "# Run the game with trained model\n",
    "def play_frogger():\n",
    "    env = gym.make('ALE/Frogger-v5', render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state = np.expand_dims(state, axis=-1)  # Add channel dimension\n",
    "    state = np.expand_dims(state, axis=0)   # Add batch dimension\n",
    "    \n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        # Get Q-values and choose best action\n",
    "        q_values = model.predict(state, verbose=0)\n",
    "        action = np.argmax(q_values[0])\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Prepare next state\n",
    "        next_state = preprocess_frame(next_state)\n",
    "        next_state = np.expand_dims(next_state, axis=-1)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Game Over! Total reward: {total_reward}\")\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "\n",
    "# Run the game\n",
    "if __name__ == \"__main__\":\n",
    "    play_frogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954158f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
