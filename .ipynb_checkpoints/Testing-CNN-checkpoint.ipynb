{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae0a1e9-eddc-4625-ad93-0a7aba4b3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada4a3d5-91ea-4efe-abf2-a67fa06b6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale_py in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from ale_py) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5be514-3f69-491e-b172-2ba7dbd662dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (4.13.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in c:\\users\\mmooreii\\appdata\\local\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.10.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e4f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import ale_py\n",
    "from tensorflow.keras import layers, models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6091137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale and resize to 120x160\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized_frame = cv2.resize(gray_frame, (160, 120))\n",
    "    normalized_frame = resized_frame / 255.0  # Normalize to [0, 1]\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9114e015-c9eb-451b-bdf6-bc8a8d714e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"CNN architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdae6fe6-eb82-45e8-8595-4f69114e5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System for progressively saving weights as the CNN trains\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcc3db75-c6d8-4986-80aa-88d8ee4ed17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(action_size)  # Random action (explore)\n",
    "    q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)  # Predict Q-values\n",
    "    return np.argmax(q_values[0])  # Action with highest Q-value (exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07bd1d54-161a-4e72-a6be-5e120f7da915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN architecture for action prediction\n",
    "def build_cnn(action_size, input_shape=(120, 160, 1)):\n",
    "    \"\"\"\n",
    "    Build a Convolutional Neural Network (CNN) for action prediction in an RL environment.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First convolutional layer\n",
    "    model.add(layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=2, activation='relu'))\n",
    "    \n",
    "    # Third convolutional layer\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=1, activation='relu'))\n",
    "    \n",
    "    # Flatten the output for the fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    \n",
    "    # Output layer with 'action_size' neurons (one for each possible action)\n",
    "    model.add(layers.Dense(action_size, activation='softmax'))  # Softmax for action probability\n",
    "\n",
    "    # Compile the model with a categorical crossentropy loss (because it's a classification task)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9eb77a63-ffbb-4202-a802-ff9c682b8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a CNN model for action prediction on the Frogger environment\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create CNN model\n",
    "    model = build_cnn(action_size)\n",
    "    \n",
    "    # Model saving directory\n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Initialize memory\n",
    "    memory = []\n",
    "    \n",
    "    # Training loop\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(1, episodes + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "\n",
    "        episode_reward = 0\n",
    "        steps_taken = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_action(model, state, epsilon, action_size)\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Reward shaping for completion speed\n",
    "            time_penalty = 0.01\n",
    "            speed_bonus = 10 if done and reward > 0 else 0\n",
    "            reward = reward - time_penalty + speed_bonus\n",
    "            reward = np.clip(reward, -10, 10)\n",
    "\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps_taken += 1\n",
    "            \n",
    "            # If memory has enough samples, train the model (optional)\n",
    "            if len(memory) > batch_size:\n",
    "                # Sample a minibatch from memory\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                \n",
    "                # Prepare training data from minibatch\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "                \n",
    "                # One-hot encode the actions for categorical crossentropy\n",
    "                actions_one_hot = tf.keras.utils.to_categorical(actions, num_classes=action_size)\n",
    "                \n",
    "                # Train the model using the states and actions\n",
    "                model.fit(states, actions_one_hot, epochs=1, verbose=0)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon for exploration-exploitation balance\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "            \n",
    "        #  Calculate duration after the episode finishes\n",
    "        end_time = time.time()\n",
    "        episode_duration = end_time - start_time\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode {episode} | Reward: {episode_reward:.2f} | Steps: {steps_taken} | Time: {episode_duration:.2f}s | Epsilon: {epsilon:.4f}\")\n",
    "        \n",
    "        # Save the model periodically\n",
    "        if episode % save_freq == 0:\n",
    "            model.save(f\"{save_dir}/frogger_cnn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(f\"{save_dir}/frogger_cnn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68dd17-e235-4ef6-8d2b-8f29c4b02f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Reward: 6.40 | Steps: 260 | Time: 32.96s | Epsilon: 0.9950\n",
      "Episode 2 | Reward: 14.26 | Steps: 275 | Time: 39.08s | Epsilon: 0.9900\n",
      "Episode 3 | Reward: 12.72 | Steps: 229 | Time: 33.24s | Epsilon: 0.9851\n",
      "Episode 4 | Reward: 4.00 | Steps: 300 | Time: 41.40s | Epsilon: 0.9801\n",
      "Episode 5 | Reward: 13.65 | Steps: 236 | Time: 32.44s | Epsilon: 0.9752\n",
      "Episode 6 | Reward: 12.72 | Steps: 229 | Time: 31.40s | Epsilon: 0.9704\n",
      "Episode 7 | Reward: 17.97 | Steps: 304 | Time: 41.87s | Epsilon: 0.9655\n",
      "Episode 8 | Reward: 6.03 | Steps: 297 | Time: 41.60s | Epsilon: 0.9607\n",
      "Episode 9 | Reward: 17.14 | Steps: 287 | Time: 42.47s | Epsilon: 0.9559\n",
      "Episode 10 | Reward: 6.94 | Steps: 306 | Time: 43.58s | Epsilon: 0.9511\n",
      "Episode 11 | Reward: 8.10 | Steps: 290 | Time: 41.41s | Epsilon: 0.9464\n",
      "Episode 12 | Reward: 4.32 | Steps: 268 | Time: 38.08s | Epsilon: 0.9416\n",
      "Episode 13 | Reward: 13.63 | Steps: 238 | Time: 33.17s | Epsilon: 0.9369\n",
      "Episode 14 | Reward: 17.16 | Steps: 285 | Time: 41.67s | Epsilon: 0.9322\n",
      "Episode 15 | Reward: 5.20 | Steps: 280 | Time: 41.21s | Epsilon: 0.9276\n",
      "Episode 16 | Reward: 7.10 | Steps: 290 | Time: 42.45s | Epsilon: 0.9229\n",
      "Episode 17 | Reward: 8.22 | Steps: 278 | Time: 40.73s | Epsilon: 0.9183\n",
      "Episode 18 | Reward: 2.44 | Steps: 256 | Time: 37.38s | Epsilon: 0.9137\n",
      "Episode 19 | Reward: 10.35 | Steps: 265 | Time: 39.21s | Epsilon: 0.9092\n",
      "Episode 20 | Reward: 9.44 | Steps: 256 | Time: 39.02s | Epsilon: 0.9046\n",
      "Episode 21 | Reward: 5.96 | Steps: 304 | Time: 45.74s | Epsilon: 0.9001\n",
      "Episode 22 | Reward: 8.39 | Steps: 261 | Time: 38.95s | Epsilon: 0.8956\n",
      "Episode 23 | Reward: 4.55 | Steps: 245 | Time: 35.73s | Epsilon: 0.8911\n",
      "Episode 24 | Reward: 17.77 | Steps: 324 | Time: 49.72s | Epsilon: 0.8867\n",
      "Episode 25 | Reward: 17.89 | Steps: 312 | Time: 48.26s | Epsilon: 0.8822\n",
      "Episode 26 | Reward: 10.23 | Steps: 277 | Time: 42.79s | Epsilon: 0.8778\n",
      "Episode 27 | Reward: 11.82 | Steps: 319 | Time: 48.91s | Epsilon: 0.8734\n",
      "Episode 28 | Reward: 15.50 | Steps: 251 | Time: 41.12s | Epsilon: 0.8691\n",
      "Episode 29 | Reward: 14.05 | Steps: 296 | Time: 44.47s | Epsilon: 0.8647\n",
      "Episode 30 | Reward: 6.96 | Steps: 304 | Time: 49.48s | Epsilon: 0.8604\n",
      "Episode 31 | Reward: 14.11 | Steps: 290 | Time: 45.17s | Epsilon: 0.8561\n",
      "Episode 32 | Reward: 7.41 | Steps: 259 | Time: 39.66s | Epsilon: 0.8518\n",
      "Episode 33 | Reward: 14.62 | Steps: 239 | Time: 36.85s | Epsilon: 0.8475\n",
      "Episode 34 | Reward: 3.83 | Steps: 217 | Time: 32.92s | Epsilon: 0.8433\n",
      "Episode 35 | Reward: 17.56 | Steps: 245 | Time: 37.77s | Epsilon: 0.8391\n",
      "Episode 36 | Reward: 15.80 | Steps: 221 | Time: 34.51s | Epsilon: 0.8349\n",
      "Episode 37 | Reward: 5.75 | Steps: 225 | Time: 36.38s | Epsilon: 0.8307\n",
      "Episode 38 | Reward: 16.41 | Steps: 260 | Time: 41.35s | Epsilon: 0.8266\n",
      "Episode 39 | Reward: 5.19 | Steps: 281 | Time: 43.94s | Epsilon: 0.8224\n",
      "Episode 40 | Reward: 15.29 | Steps: 272 | Time: 41.99s | Epsilon: 0.8183\n",
      "Episode 41 | Reward: 5.25 | Steps: 275 | Time: 44.71s | Epsilon: 0.8142\n",
      "Episode 42 | Reward: 4.42 | Steps: 258 | Time: 44.54s | Epsilon: 0.8102\n",
      "Episode 43 | Reward: 7.01 | Steps: 299 | Time: 49.09s | Epsilon: 0.8061\n",
      "Episode 44 | Reward: 8.29 | Steps: 371 | Time: 60.93s | Epsilon: 0.8021\n",
      "Episode 45 | Reward: 9.31 | Steps: 269 | Time: 42.86s | Epsilon: 0.7981\n",
      "Episode 46 | Reward: 19.04 | Steps: 297 | Time: 48.66s | Epsilon: 0.7941\n",
      "Episode 47 | Reward: 14.84 | Steps: 217 | Time: 35.26s | Epsilon: 0.7901\n",
      "Episode 48 | Reward: 9.26 | Steps: 274 | Time: 43.69s | Epsilon: 0.7862\n",
      "Episode 49 | Reward: 6.65 | Steps: 235 | Time: 38.54s | Epsilon: 0.7822\n",
      "Episode 50 | Reward: 4.54 | Steps: 346 | Time: 55.88s | Epsilon: 0.7783\n",
      "Episode 51 | Reward: 11.52 | Steps: 348 | Time: 54.44s | Epsilon: 0.7744\n",
      "Episode 52 | Reward: 15.50 | Steps: 251 | Time: 41.24s | Epsilon: 0.7705\n",
      "Episode 53 | Reward: 16.19 | Steps: 282 | Time: 46.41s | Epsilon: 0.7667\n",
      "Episode 54 | Reward: 14.19 | Steps: 282 | Time: 45.62s | Epsilon: 0.7629\n",
      "Episode 55 | Reward: 7.38 | Steps: 362 | Time: 59.54s | Epsilon: 0.7590\n",
      "Episode 56 | Reward: 14.93 | Steps: 308 | Time: 50.39s | Epsilon: 0.7553\n",
      "Episode 57 | Reward: 16.34 | Steps: 267 | Time: 44.24s | Epsilon: 0.7515\n",
      "Episode 58 | Reward: 8.03 | Steps: 297 | Time: 50.97s | Epsilon: 0.7477\n",
      "Episode 59 | Reward: 4.95 | Steps: 305 | Time: 50.41s | Epsilon: 0.7440\n",
      "Episode 60 | Reward: 6.09 | Steps: 291 | Time: 49.79s | Epsilon: 0.7403\n",
      "Episode 61 | Reward: 3.20 | Steps: 280 | Time: 47.03s | Epsilon: 0.7366\n",
      "Episode 62 | Reward: 14.83 | Steps: 318 | Time: 52.57s | Epsilon: 0.7329\n",
      "Episode 63 | Reward: 14.99 | Steps: 302 | Time: 52.30s | Epsilon: 0.7292\n",
      "Episode 64 | Reward: 8.17 | Steps: 283 | Time: 48.66s | Epsilon: 0.7256\n",
      "Episode 65 | Reward: 16.09 | Steps: 292 | Time: 51.11s | Epsilon: 0.7219\n",
      "Episode 66 | Reward: 5.83 | Steps: 217 | Time: 35.53s | Epsilon: 0.7183\n",
      "Episode 67 | Reward: 4.83 | Steps: 317 | Time: 53.34s | Epsilon: 0.7147\n",
      "Episode 68 | Reward: 14.28 | Steps: 273 | Time: 46.23s | Epsilon: 0.7112\n",
      "Episode 69 | Reward: 11.16 | Steps: 384 | Time: 67.14s | Epsilon: 0.7076\n",
      "Episode 70 | Reward: 15.05 | Steps: 296 | Time: 51.24s | Epsilon: 0.7041\n",
      "Episode 71 | Reward: 15.12 | Steps: 289 | Time: 50.49s | Epsilon: 0.7005\n",
      "Episode 72 | Reward: 17.07 | Steps: 294 | Time: 53.02s | Epsilon: 0.6970\n",
      "Episode 73 | Reward: 15.52 | Steps: 249 | Time: 44.58s | Epsilon: 0.6936\n",
      "Episode 74 | Reward: 15.93 | Steps: 308 | Time: 54.00s | Epsilon: 0.6901\n",
      "Episode 75 | Reward: 18.04 | Steps: 297 | Time: 52.08s | Epsilon: 0.6866\n",
      "Episode 76 | Reward: 15.12 | Steps: 289 | Time: 50.83s | Epsilon: 0.6832\n",
      "Episode 77 | Reward: 15.04 | Steps: 297 | Time: 52.64s | Epsilon: 0.6798\n",
      "Episode 78 | Reward: 7.93 | Steps: 307 | Time: 55.92s | Epsilon: 0.6764\n",
      "Episode 79 | Reward: 12.94 | Steps: 307 | Time: 54.44s | Epsilon: 0.6730\n",
      "Episode 80 | Reward: 14.45 | Steps: 256 | Time: 45.58s | Epsilon: 0.6696\n",
      "Episode 81 | Reward: 6.19 | Steps: 281 | Time: 50.34s | Epsilon: 0.6663\n",
      "Episode 82 | Reward: 6.35 | Steps: 265 | Time: 47.68s | Epsilon: 0.6630\n",
      "Episode 83 | Reward: 13.93 | Steps: 308 | Time: 56.69s | Epsilon: 0.6597\n",
      "Episode 84 | Reward: 18.96 | Steps: 305 | Time: 59.77s | Epsilon: 0.6564\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_cnn(episodes=1000, max_steps=50000, batch_size=32)\n",
    "#Pray it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ddd07-55d3-4644-be76-073cac91ef78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
