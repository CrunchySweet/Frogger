{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e4f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6091137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB data to grayscale. Keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9114e015-c9eb-451b-bdf6-bc8a8d714e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"CNN architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdae6fe6-eb82-45e8-8595-4f69114e5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System for progressively saving weights as the CNN trains\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc3db75-c6d8-4986-80aa-88d8ee4ed17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy -- balances exploration and exploitation\n",
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.random() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose random action\n",
    "    else:\n",
    "        # Exploit: choose best action\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)  # Choose action with highest Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb77a63-ffbb-4202-a802-ff9c682b8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_dqn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=10000,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a DQN model on Frogger\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create main and target models\n",
    "    main_model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(main_model.get_weights())  # Initialize target with same weights\n",
    "    \n",
    "    # Create memory for experience replay\n",
    "    memory = create_memory(capacity=memory_capacity)\n",
    "    \n",
    "    # Training metrics\n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Model saving directory\n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, episodes + 1):\n",
    "        # Reset environment and get initial state\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension: (120, 160, 1)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = epsilon_greedy_action(main_model, state, epsilon, action_size)\n",
    "            \n",
    "            # Take action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)  # Add channel dimension\n",
    "            \n",
    "            # Store experience in memory\n",
    "            add_to_memory(memory, state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Train with experience replay if memory has enough samples\n",
    "            if len(memory) > batch_size:\n",
    "                # Sample batch from memory\n",
    "                minibatch = sample_from_memory(memory, batch_size)\n",
    "                \n",
    "                # Prepare batch for training\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                target_q_values = main_model.predict(states, verbose=0)\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        target_q_values[i, actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target_q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "                \n",
    "                # Train the model\n",
    "                main_model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "            \n",
    "            # Update target network periodically\n",
    "            if total_steps % update_target_freq == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"Target network updated at step {total_steps}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}, Steps: {step+1}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % save_freq == 0:\n",
    "            main_model.save(f\"{save_dir}/frogger_dqn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    # Save final model\n",
    "    main_model.save(f\"{save_dir}/frogger_dqn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68dd17-e235-4ef6-8d2b-8f29c4b02f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as needed\n",
    "    train_dqn(episodes=1000,\n",
    "              max_steps=10000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.01, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=1000,\n",
    "              memory_capacity=50000,\n",
    "              save_freq=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
